{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soulayman-git/test-sub-branch/blob/master/TP3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression Lineare**"
      ],
      "metadata": {
        "id": "7ZVw7vbD_Ek5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Methode normal**\n"
      ],
      "metadata": {
        "id": "vfVD5zx0FRS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Générer des données synthétiques (t_donnees et y)\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 100)  # Vecteur des valeurs de t_donnees\n",
        "a_vrai, b_vrai = 2, 3  # paramètres réels\n",
        "y_donnees = a_vrai * t_donnees + b_vrai + np.random.normal(0, 1, len(t_donnees))  # y_donnees = ax + b avec du bruit\n",
        "\n",
        "# Appliquer la méthode normale\n",
        "X = np.vstack((t_donnees, np.ones_like(t_donnees))).T  # Ajouter une colonne de 1 pour l'ordonnée à l'origine\n",
        "theta = np.linalg.inv(X.T @ X) @ X.T @ y_donnees  # Résoudre pour les paramètres (a, b)\n",
        "\n",
        "a_normale, b_normale = theta  # Extraire les coefficients\n",
        "\n",
        "# Afficher les résultats\n",
        "print(f\"Valeurs estimées par la méthode normale : a = {a_normale}, b = {b_normale}\")\n",
        "print(f\"Valeurs réelles : a = {a_vrai}, b = {b_vrai}\")\n",
        "\n",
        "# Tracer les résultats\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Données bruitées\", color='blue', alpha=0.5)  # Utilisation correcte de t_donnees\n",
        "plt.plot(t_donnees, a_vrai * t_donnees + b_vrai, label=\"Modèle réel\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, a_normale * t_donnees + b_normale, label=\"Modèle Méthode Normale\", color='red')\n",
        "plt.title(\"Régression Linéaire avec la Méthode Normale\")\n",
        "plt.xlabel(\"t_donnees\")\n",
        "plt.ylabel(\"y_donnees\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gai7WdeZBESa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methode garadiant descente pas fix, pas optimal"
      ],
      "metadata": {
        "id": "06xxEDFmG-0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Générer des données synthétiques pour la régression linéaire\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 100)\n",
        "a_vrai, b_vrai = 2, 1\n",
        "y_donnees = a_vrai * t_donnees + b_vrai + np.random.normal(0, 1, len(t_donnees))\n",
        "\n",
        "# Fonction de perte (Erreur quadratique moyenne)\n",
        "def fonction_perte(t, y, a, b):\n",
        "    predictions = a * t + b\n",
        "    return np.mean((predictions - y) ** 2)\n",
        "\n",
        "# Gradient de la fonction de perte\n",
        "def gradients(t, y, a, b):\n",
        "    grad_a = -2 * np.mean((y - (a * t + b)) * t)\n",
        "    grad_b = -2 * np.mean(y - (a * t + b))\n",
        "    return grad_a, grad_b\n",
        "\n",
        "# 1. Descente de gradient avec pas fixe\n",
        "def descente_gradient_pas_fixe(t, y, a_init, b_init, taux_apprentissage, n_iterations):\n",
        "    a, b = a_init, b_init\n",
        "    historique_perte = []\n",
        "    for i in range(n_iterations):\n",
        "        grad_a, grad_b = gradients(t, y, a, b)\n",
        "        a -= taux_apprentissage * grad_a\n",
        "        b -= taux_apprentissage * grad_b\n",
        "        historique_perte.append(fonction_perte(t, y, a, b))\n",
        "    return a, b, historique_perte\n",
        "\n",
        "# 2. Recherche de pas optimal\n",
        "def recherche_pas_optimal(t, y, a, b, grad_a, grad_b, taux_apprentissage=1.0):\n",
        "    alpha = 0.5  # facteur de réduction\n",
        "    beta = 0.1   # tolérance\n",
        "    while fonction_perte(t, y, a - taux_apprentissage * grad_a, b - taux_apprentissage * grad_b) > fonction_perte(t, y, a, b) - beta * taux_apprentissage * (grad_a**2 + grad_b**2):\n",
        "        taux_apprentissage *= alpha\n",
        "    return taux_apprentissage\n",
        "\n",
        "def descente_gradient_recherche_pas_optimal(t, y, a_init, b_init, n_iterations):\n",
        "    a, b = a_init, b_init\n",
        "    historique_perte = []\n",
        "    for i in range(n_iterations):\n",
        "        grad_a, grad_b = gradients(t, y, a, b)\n",
        "        taux_apprentissage = recherche_pas_optimal(t, y, a, b, grad_a, grad_b)\n",
        "        a -= taux_apprentissage * grad_a\n",
        "        b -= taux_apprentissage * grad_b\n",
        "        historique_perte.append(fonction_perte(t, y, a, b))\n",
        "    return a, b, historique_perte\n",
        "\n",
        "# 3. Gradient conjugué\n",
        "def gradient_conjugue(t, y, a_init, b_init, n_iterations):\n",
        "    a, b = a_init, b_init\n",
        "    grad_a, grad_b = gradients(t, y, a, b)\n",
        "    r = np.array([grad_a, grad_b])  # Résidu initial\n",
        "    p = r.copy()  # Direction de recherche initiale\n",
        "    historique_perte = []\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        alpha = np.dot(r, r) / np.dot(p, gradients(t, y, a, b))  # Calcul de alpha\n",
        "        a -= alpha * grad_a\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        grad_a, grad_b = gradients(t, y, a, b)\n",
        "        r_new = np.array([grad_a, grad_b])\n",
        "\n",
        "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
        "        p = r_new + beta * p  # Mise à jour de la direction de recherche\n",
        "        r = r_new\n",
        "\n",
        "        historique_perte.append(fonction_perte(t, y, a, b))\n",
        "\n",
        "# Appliquer les différentes méthodes\n",
        "a_init, b_init = 1, 1\n",
        "n_iterations = 1000"
      ],
      "metadata": {
        "id": "uBPKlOjn_H1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descente de gradient avec pas fixe\n",
        "a_gd_fs, b_gd_fs, perte_gd_fs = descente_gradient_pas_fixe(t_donnees, y_donnees, a_init, b_init, taux_apprentissage=0.01, n_iterations=n_iterations)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Données\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, a_vrai * t_donnees + b_vrai, label=\"Modèle réel\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, a_gd_fs * t_donnees + b_gd_fs, label=\"Descente de Gradient avec Pas Fixe\", color='red')\n",
        "\n",
        "print(f\"Valeurs estimées : Descente de Gradient avec Pas Fixe -> a = {a_gd_fs}, b = {b_gd_fs}\")\n",
        "plt.title(\"Methode gradiant descent pas fix\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Temps (t)\")\n",
        "plt.ylabel(\"Valeurs (y)\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D_07TFKo_I0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descente de gradient avec recherche de pas optimal\n",
        "a_gd_rpo, b_gd_rpo, perte_gd_rpo = descente_gradient_recherche_pas_optimal(t_donnees, y_donnees, a_init, b_init, n_iterations)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Données\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, a_vrai * t_donnees + b_vrai, label=\"Modèle réel\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, a_gd_rpo * t_donnees + b_gd_rpo, label=\"Descente de Gradient avec Recherche de Pas Optimal\", color='purple')\n",
        "\n",
        "plt.title(\"Methode gradiant descent pas optimal\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Temps (t)\")\n",
        "plt.ylabel(\"Valeurs (y)\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Valeurs estimées : Descente de Gradient avec Recherche de Pas Optimal -> a = {a_gd_rpo}, b = {b_gd_rpo}\")"
      ],
      "metadata": {
        "id": "gJYK1T4zAnOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression non linear**"
      ],
      "metadata": {
        "id": "YkDalH-gLfpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methode Normal"
      ],
      "metadata": {
        "id": "VrCY9kmj80ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import curve_fit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fonction modèle\n",
        "def model(t, a, b, c, d):\n",
        "    return a * np.exp(-b * t) * np.cos(c * t + d)\n",
        "\n",
        "# Données simulées (par exemple)\n",
        "t_data = np.linspace(0, 10, 100)  # valeurs de t de 0 à 10\n",
        "y_data = model(t_data, 2, 0.5, 1, 0) + 0.1 * np.random.normal(size=len(t_data))  # y avec un peu de bruit\n",
        "\n",
        "# Estimation des paramètres avec des valeurs initiales\n",
        "initial_guess = [1, 1, 1, 1]\n",
        "params, covariance = curve_fit(model, t_data, y_data, p0=initial_guess) # minimiser la différence entre les prédictions du modèle et les données réelles\n",
        "\n",
        "# Afficher les paramètres optimisés\n",
        "a_opt, b_opt, c_opt, d_opt = params\n",
        "print(f\"Paramètres optimisés : a = {a_opt}, b = {b_opt}, c = {c_opt}, d = {d_opt}\")\n",
        "\n",
        "# Tracer les données, la courbe ajustée, et le modèle réel\n",
        "y_fit = model(t_data, a_opt, b_opt, c_opt, d_opt)\n",
        "y_real = model(t_data, 2, 0.5, 1, 0)  # Modèle réel sans bruit\n",
        "plt.title(\"Methode normal\")\n",
        "plt.plot(t_data, y_data, 'o', label='Données', color='blue')\n",
        "plt.plot(t_data, y_real, '--', label='Modèle réel', color='black')\n",
        "plt.plot(t_data, y_fit, '-', label='Modèle ajusté', color='red')\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Nxw2bFB1LJrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas fix"
      ],
      "metadata": {
        "id": "Pp8QWRgTu6Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Définir le modèle\n",
        "def modele(t, a, b, c, d):\n",
        "    return a * np.exp(-b * t) * np.cos(c * t + d)\n",
        "\n",
        "# Fonction de perte (Erreur quadratique moyenne)\n",
        "def fonction_perte(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    return np.mean((predictions - y) ** 2)\n",
        "\n",
        "# Gradient de la fonction de perte\n",
        "def gradients(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    grad_a = 2 * np.mean((predictions - y) * np.exp(-b * t) * np.cos(c * t + d))\n",
        "    grad_b = 2 * np.mean((predictions - y) * (-a * t * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    grad_c = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * t * np.sin(c * t + d)))\n",
        "    grad_d = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    return grad_a, grad_b, grad_c, grad_d\n",
        "\n",
        "# Descente de gradient avec pas fixe\n",
        "def descente_gradient_pas_fixe(t, y, a_init, b_init, c_init, d_init, taux_apprentissage, n_iterations):\n",
        "    a, b, c, d = a_init, b_init, c_init, d_init\n",
        "    historique_perte = []\n",
        "    for i in range(n_iterations):\n",
        "        grad_a, grad_b, grad_c, grad_d = gradients(t, y, a, b, c, d)\n",
        "        a -= taux_apprentissage * grad_a\n",
        "        b -= taux_apprentissage * grad_b\n",
        "        c -= taux_apprentissage * grad_c\n",
        "        d -= taux_apprentissage * grad_d\n",
        "        historique_perte.append(fonction_perte(t, y, a, b, c, d))\n",
        "    return a, b, c, d, historique_perte\n",
        "\n",
        "# Générer des données synthétiques\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 300)\n",
        "a_vrai, b_vrai, c_vrai, d_vrai = 5, 1, 2, 0.5\n",
        "y_donnees = modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai) + np.random.normal(0, 0.5, len(t_donnees))\n",
        "\n",
        "# Appliquer la descente de gradient avec pas fixe\n",
        "a_gd, b_gd, c_gd, d_gd, perte_gd = descente_gradient_pas_fixe(\n",
        "    t_donnees, y_donnees, a_init=1, b_init=1, c_init=1, d_init=1, taux_apprentissage=0.01, n_iterations=1000\n",
        ")\n",
        "\n",
        "# Tracer les résultats\n",
        "print(f\"Valeurs estimées : a = {a_gd}, b = {b_gd}, c = {c_gd}, d = {d_gd}\")\n",
        "print(f\"Valeurs reels : a = {a_vrai}, b = {b_vrai}, c = {c_vrai}, d = {d_vrai}\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Données bruitées\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai), label=\"Modèle réel\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, modele(t_donnees, a_gd, b_gd, c_gd, d_gd), label=\"Modèle (Gradient Descent)\", color='red')\n",
        "plt.title(\"Methode gradiant descent pas fix\")\n",
        "plt.xlabel(\"Temps (t)\")\n",
        "plt.ylabel(\"Valeurs (y)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p4N2pVCUoplG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il y a plusieurs raisons pour lesquelles le modèle estimé par la descente de gradient (en rouge) ne correspond pas exactement au modèle réel (en noir). Voici les principales explications :\n",
        "\n",
        "1. Bruit dans les Données\n",
        "Les données utilisées pour entraîner le modèle contiennent du bruit (ajouté aléatoirement avec np.random.normal). Cela peut rendre l'ajustement des paramètres plus difficile.\n",
        "Le bruit peut empêcher la descente de gradient de converger parfaitement vers les valeurs réelles des paramètres.\n",
        "2. Initialisation des Paramètres\n",
        "La descente de gradient démarre à partir d'une estimation initiale des paramètres (a=1, b=1, c=1, d=1). Si ces valeurs sont loin des paramètres réels (\n",
        "a=5, b=1, c=2, d=0.5), la convergence peut être lente ou partielle.\n",
        "Une mauvaise initialisation peut piéger l'algorithme dans un minimum local.\n",
        "3. Taux d'Apprentissage\n",
        "Le taux d'apprentissage (𝜂=0.01) utilisé dans cet exemple est fixe. Si ce taux est trop petit, la convergence est lente et peut ne pas atteindre le minimum global dans le nombre d'itérations spécifié.\n",
        "Si le taux est trop grand, il peut provoquer des oscillations autour du minimum ou empêcher la convergence.\n",
        "4. Nombre d'Itérations\n",
        "Le nombre d'itérations (1000) peut être insuffisant pour que l'algorithme converge complètement, surtout si le gradient diminue lentement au fur et à mesure de l'ajustement des paramètres.\n",
        "5. Non-linéarité et Complexité de la Fonction\n",
        "La fonction f(t)=aexp(−bt)cos(ct+d) est hautement non-linéaire par rapport à ses paramètres. Cela rend la descente de gradient plus difficile, car l'algorithme peut avoir du mal à trouver la direction optimale dans cet espace non-linéaire.\n",
        "6. Pas d'Approches Avancées\n",
        "Les méthodes comme momentum, Adam, ou encore la descente avec pas optimal pourraient améliorer la performance de la descente de gradient et permettre une meilleure convergence vers les paramètres réels.\n"
      ],
      "metadata": {
        "id": "n72FFFFgva-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pas Optimal**"
      ],
      "metadata": {
        "id": "wMMRA9WzxhVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Définir le modèle\n",
        "def modele(t, a, b, c, d):\n",
        "    return a * np.exp(-b * t) * np.cos(c * t + d)\n",
        "\n",
        "# Fonction de perte (Erreur quadratique moyenne)\n",
        "def fonction_perte(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    return np.mean((predictions - y) ** 2)\n",
        "\n",
        "# Gradient de la fonction de perte\n",
        "def gradients(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    grad_a = 2 * np.mean((predictions - y) * np.exp(-b * t) * np.cos(c * t + d))\n",
        "    grad_b = 2 * np.mean((predictions - y) * (-a * t * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    grad_c = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * t * np.sin(c * t + d)))\n",
        "    grad_d = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    return grad_a, grad_b, grad_c, grad_d\n",
        "\n",
        "# Recherche pas optimal pour ajuster le taux d'apprentissage\n",
        "def recherche_pas_optimal(t, y, a, b, c, d, grad_a, grad_b, grad_c, grad_d, taux_apprentissage=1.0):\n",
        "    alpha = 0.5  # facteur de réduction\n",
        "    beta = 0.1   # tolérance\n",
        "    while fonction_perte(t, y, a - taux_apprentissage * grad_a, b - taux_apprentissage * grad_b, c - taux_apprentissage * grad_c, d - taux_apprentissage * grad_d) > fonction_perte(t, y, a, b, c, d) - beta * taux_apprentissage * (grad_a**2 + grad_b**2 + grad_c**2 + grad_d**2):\n",
        "        taux_apprentissage *= alpha\n",
        "    return taux_apprentissage\n",
        "\n",
        "def descente_gradient_recherche_pas_optimal(t, y, a_init, b_init, c_init, d_init, n_iterations):\n",
        "    a, b, c, d = a_init, b_init, c_init, d_init\n",
        "    historique_perte = []\n",
        "    for i in range(n_iterations):\n",
        "        grad_a, grad_b, grad_c, grad_d = gradients(t, y, a, b, c, d)\n",
        "        taux_apprentissage = recherche_pas_optimal(t, y, a, b, c, d, grad_a, grad_b, grad_c, grad_d)\n",
        "        a -= taux_apprentissage * grad_a\n",
        "        b -= taux_apprentissage * grad_b\n",
        "        c -= taux_apprentissage * grad_c\n",
        "        d -= taux_apprentissage * grad_d\n",
        "        historique_perte.append(fonction_perte(t, y, a, b, c, d))\n",
        "    return a, b, c, d, historique_perte\n",
        "\n",
        "# Générer des données synthétiques basées sur le modèle\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 100)\n",
        "a_vrai, b_vrai, c_vrai, d_vrai = 5, 1, 2, 0.5\n",
        "y_donnees = modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai) + np.random.normal(0, 0.5, len(t_donnees))  # Ajouter du bruit\n",
        "\n",
        "# Appliquer la descente de gradient avec recherche de ligne\n",
        "a_gd_ls, b_gd_ls, c_gd_ls, d_gd_ls, perte_gd_ls = descente_gradient_recherche_pas_optimal(t_donnees, y_donnees, a_init=1, b_init=1, c_init=1, d_init=1, n_iterations=1000)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(f\"Valeurs estimées : a = {a_gd}, b = {b_gd}, c = {c_gd}, d = {d_gd}\")\n",
        "print(f\"Valeurs reels : a = {a_vrai}, b = {b_vrai}, c = {c_vrai}, d = {d_vrai}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Données\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai), label=\"Modèle réel\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, modele(t_donnees, a_gd_ls, b_gd_ls, c_gd_ls, d_gd_ls), label=\"Descente de Gradient Pas optimal\", color='red')\n",
        "plt.title(\"Methode gradiant descent pas optimal\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H2uhbMcEvIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conjugee"
      ],
      "metadata": {
        "id": "J3It71QhZN6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Définir le modèle\n",
        "def modele(t, a, b, c, d):\n",
        "    return a * np.exp(-b * t) * np.cos(c * t + d)\n",
        "\n",
        "# Fonction de perte (Erreur quadratique moyenne)\n",
        "def fonction_perte(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    return np.mean((predictions - y) ** 2)\n",
        "\n",
        "# Gradient de la fonction de perte\n",
        "def gradients(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    grad_a = 2 * np.mean((predictions - y) * np.exp(-b * t) * np.cos(c * t + d))\n",
        "    grad_b = 2 * np.mean((predictions - y) * (-a * t * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    grad_c = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * t * np.sin(c * t + d)))\n",
        "    grad_d = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    return grad_a, grad_b, grad_c, grad_d\n",
        "\n",
        "# Méthode du Gradient Conjugué\n",
        "def gradient_conjugue(t, y, a_init, b_init, c_init, d_init, n_iterations):\n",
        "    # Initialisation des paramètres\n",
        "    a, b, c, d = a_init, b_init, c_init, d_init\n",
        "    grad_a, grad_b, grad_c, grad_d = gradients(t, y, a, b, c, d)\n",
        "    r = np.array([grad_a, grad_b, grad_c, grad_d])  # Résidu initial (gradient)\n",
        "    p = r.copy()  # Direction de recherche initiale\n",
        "    historique_perte = []\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        # Calcul de alpha (taille du pas)\n",
        "        alpha = np.dot(r, r) / np.dot(p, gradients(t, y, a, b, c, d))\n",
        "\n",
        "        # Mise à jour des paramètres\n",
        "        a -= alpha * grad_a\n",
        "        b -= alpha * grad_b\n",
        "        c -= alpha * grad_c\n",
        "        d -= alpha * grad_d\n",
        "\n",
        "        # Nouveau résidu\n",
        "        grad_a, grad_b, grad_c, grad_d = gradients(t, y, a, b, c, d)\n",
        "        r_new = np.array([grad_a, grad_b, grad_c, grad_d])\n",
        "\n",
        "        # Calcul du facteur beta pour la mise à jour de la direction\n",
        "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
        "        p = r_new + beta * p  # Mise à jour de la direction de recherche\n",
        "        r = r_new\n",
        "        historique_perte.append(fonction_perte(t, y, a, b, c, d))\n",
        "    return a, b, c, d, historique_perte\n",
        "\n",
        "# Générer des données synthétiques basées sur le modèle\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 100)\n",
        "a_vrai, b_vrai, c_vrai, d_vrai = 5, 1, 2, 0.5\n",
        "y_donnees = modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai) + np.random.normal(0, 0.5, len(t_donnees))  # Ajouter du bruit\n",
        "\n",
        "# Appliquer la méthode du gradient conjugué\n",
        "a_cg, b_cg, c_cg, d_cg, perte_cg = gradient_conjugue(t_donnees, y_donnees, a_init=1, b_init=1, c_init=1, d_init=1, n_iterations=10000)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(f\"Valeurs estimées : a = {a_cg}, b = {b_cg}, c = {c_cg}, d = {d_cg}\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Données\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai), label=\"Modèle réel\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, modele(t_donnees, a_cg, b_cg, c_cg, d_cg), label=\"Gradient Conjugué\", color='red')\n",
        "plt.title(\"Méthode du Gradient Conjugué\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Aq-roIa5ZPq1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "441d28e3-a72e-40ba-a5ee-8ed332032cc4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'a_gd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c39eee011d5e>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Afficher les résultats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Valeurs estimées : a = {a_gd}, b = {b_gd}, c = {c_gd}, d = {d_gd}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_donnees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_donnees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Données\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'a_gd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22YJ_rWYr1w3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}