{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soulayman-git/test-sub-branch/blob/master/TP3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression Lineare**"
      ],
      "metadata": {
        "id": "7ZVw7vbD_Ek5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Methode normal**\n"
      ],
      "metadata": {
        "id": "vfVD5zx0FRS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# G√©n√©rer des donn√©es synth√©tiques (t_donnees et y)\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 100)  # Vecteur des valeurs de t_donnees\n",
        "a_vrai, b_vrai = 2, 3  # param√®tres r√©els\n",
        "y_donnees = a_vrai * t_donnees + b_vrai + np.random.normal(0, 1, len(t_donnees))  # y_donnees = ax + b avec du bruit\n",
        "\n",
        "# Appliquer la m√©thode normale\n",
        "X = np.vstack((t_donnees, np.ones_like(t_donnees))).T  # Ajouter une colonne de 1 pour l'ordonn√©e √† l'origine\n",
        "theta = np.linalg.inv(X.T @ X) @ X.T @ y_donnees  # R√©soudre pour les param√®tres (a, b)\n",
        "\n",
        "a_normale, b_normale = theta  # Extraire les coefficients\n",
        "\n",
        "# Afficher les r√©sultats\n",
        "print(f\"Valeurs estim√©es par la m√©thode normale : a = {a_normale}, b = {b_normale}\")\n",
        "print(f\"Valeurs r√©elles : a = {a_vrai}, b = {b_vrai}\")\n",
        "\n",
        "# Tracer les r√©sultats\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Donn√©es bruit√©es\", color='blue', alpha=0.5)  # Utilisation correcte de t_donnees\n",
        "plt.plot(t_donnees, a_vrai * t_donnees + b_vrai, label=\"Mod√®le r√©el\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, a_normale * t_donnees + b_normale, label=\"Mod√®le M√©thode Normale\", color='red')\n",
        "plt.title(\"R√©gression Lin√©aire avec la M√©thode Normale\")\n",
        "plt.xlabel(\"t_donnees\")\n",
        "plt.ylabel(\"y_donnees\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gai7WdeZBESa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methode garadiant descente pas fix, pas optimal"
      ],
      "metadata": {
        "id": "06xxEDFmG-0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# G√©n√©rer des donn√©es synth√©tiques pour la r√©gression lin√©aire\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 100)\n",
        "a_vrai, b_vrai = 2, 1\n",
        "y_donnees = a_vrai * t_donnees + b_vrai + np.random.normal(0, 1, len(t_donnees))\n",
        "\n",
        "# Fonction de perte (Erreur quadratique moyenne)\n",
        "def fonction_perte(t, y, a, b):\n",
        "    predictions = a * t + b\n",
        "    return np.mean((predictions - y) ** 2)\n",
        "\n",
        "# Gradient de la fonction de perte\n",
        "def gradients(t, y, a, b):\n",
        "    grad_a = -2 * np.mean((y - (a * t + b)) * t)\n",
        "    grad_b = -2 * np.mean(y - (a * t + b))\n",
        "    return grad_a, grad_b\n",
        "\n",
        "# 1. Descente de gradient avec pas fixe\n",
        "def descente_gradient_pas_fixe(t, y, a_init, b_init, taux_apprentissage, n_iterations):\n",
        "    a, b = a_init, b_init\n",
        "    historique_perte = []\n",
        "    for i in range(n_iterations):\n",
        "        grad_a, grad_b = gradients(t, y, a, b)\n",
        "        a -= taux_apprentissage * grad_a\n",
        "        b -= taux_apprentissage * grad_b\n",
        "        historique_perte.append(fonction_perte(t, y, a, b))\n",
        "    return a, b, historique_perte\n",
        "\n",
        "# 2. Recherche de pas optimal\n",
        "def recherche_pas_optimal(t, y, a, b, grad_a, grad_b, taux_apprentissage=1.0):\n",
        "    alpha = 0.5  # facteur de r√©duction\n",
        "    beta = 0.1   # tol√©rance\n",
        "    while fonction_perte(t, y, a - taux_apprentissage * grad_a, b - taux_apprentissage * grad_b) > fonction_perte(t, y, a, b) - beta * taux_apprentissage * (grad_a**2 + grad_b**2):\n",
        "        taux_apprentissage *= alpha\n",
        "    return taux_apprentissage\n",
        "\n",
        "def descente_gradient_recherche_pas_optimal(t, y, a_init, b_init, n_iterations):\n",
        "    a, b = a_init, b_init\n",
        "    historique_perte = []\n",
        "    for i in range(n_iterations):\n",
        "        grad_a, grad_b = gradients(t, y, a, b)\n",
        "        taux_apprentissage = recherche_pas_optimal(t, y, a, b, grad_a, grad_b)\n",
        "        a -= taux_apprentissage * grad_a\n",
        "        b -= taux_apprentissage * grad_b\n",
        "        historique_perte.append(fonction_perte(t, y, a, b))\n",
        "    return a, b, historique_perte\n",
        "\n",
        "# 3. Gradient conjugu√©\n",
        "def gradient_conjugue(t, y, a_init, b_init, n_iterations):\n",
        "    a, b = a_init, b_init\n",
        "    grad_a, grad_b = gradients(t, y, a, b)\n",
        "    r = np.array([grad_a, grad_b])  # R√©sidu initial\n",
        "    p = r.copy()  # Direction de recherche initiale\n",
        "    historique_perte = []\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        alpha = np.dot(r, r) / np.dot(p, gradients(t, y, a, b))  # Calcul de alpha\n",
        "        a -= alpha * grad_a\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        grad_a, grad_b = gradients(t, y, a, b)\n",
        "        r_new = np.array([grad_a, grad_b])\n",
        "\n",
        "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
        "        p = r_new + beta * p  # Mise √† jour de la direction de recherche\n",
        "        r = r_new\n",
        "\n",
        "        historique_perte.append(fonction_perte(t, y, a, b))\n",
        "\n",
        "# Appliquer les diff√©rentes m√©thodes\n",
        "a_init, b_init = 1, 1\n",
        "n_iterations = 1000"
      ],
      "metadata": {
        "id": "uBPKlOjn_H1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descente de gradient avec pas fixe\n",
        "a_gd_fs, b_gd_fs, perte_gd_fs = descente_gradient_pas_fixe(t_donnees, y_donnees, a_init, b_init, taux_apprentissage=0.01, n_iterations=n_iterations)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Donn√©es\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, a_vrai * t_donnees + b_vrai, label=\"Mod√®le r√©el\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, a_gd_fs * t_donnees + b_gd_fs, label=\"Descente de Gradient avec Pas Fixe\", color='red')\n",
        "\n",
        "print(f\"Valeurs estim√©es : Descente de Gradient avec Pas Fixe -> a = {a_gd_fs}, b = {b_gd_fs}\")\n",
        "plt.title(\"Methode gradiant descent pas fix\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Temps (t)\")\n",
        "plt.ylabel(\"Valeurs (y)\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D_07TFKo_I0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descente de gradient avec recherche de pas optimal\n",
        "a_gd_rpo, b_gd_rpo, perte_gd_rpo = descente_gradient_recherche_pas_optimal(t_donnees, y_donnees, a_init, b_init, n_iterations)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Donn√©es\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, a_vrai * t_donnees + b_vrai, label=\"Mod√®le r√©el\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, a_gd_rpo * t_donnees + b_gd_rpo, label=\"Descente de Gradient avec Recherche de Pas Optimal\", color='purple')\n",
        "\n",
        "plt.title(\"Methode gradiant descent pas optimal\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Temps (t)\")\n",
        "plt.ylabel(\"Valeurs (y)\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Valeurs estim√©es : Descente de Gradient avec Recherche de Pas Optimal -> a = {a_gd_rpo}, b = {b_gd_rpo}\")"
      ],
      "metadata": {
        "id": "gJYK1T4zAnOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression non linear**"
      ],
      "metadata": {
        "id": "YkDalH-gLfpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methode Normal"
      ],
      "metadata": {
        "id": "VrCY9kmj80ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import curve_fit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fonction mod√®le\n",
        "def model(t, a, b, c, d):\n",
        "    return a * np.exp(-b * t) * np.cos(c * t + d)\n",
        "\n",
        "# Donn√©es simul√©es (par exemple)\n",
        "t_data = np.linspace(0, 10, 100)  # valeurs de t de 0 √† 10\n",
        "y_data = model(t_data, 2, 0.5, 1, 0) + 0.1 * np.random.normal(size=len(t_data))  # y avec un peu de bruit\n",
        "\n",
        "# Estimation des param√®tres avec des valeurs initiales\n",
        "initial_guess = [1, 1, 1, 1]\n",
        "params, covariance = curve_fit(model, t_data, y_data, p0=initial_guess) # minimiser la diff√©rence entre les pr√©dictions du mod√®le et les donn√©es r√©elles\n",
        "\n",
        "# Afficher les param√®tres optimis√©s\n",
        "a_opt, b_opt, c_opt, d_opt = params\n",
        "print(f\"Param√®tres optimis√©s : a = {a_opt}, b = {b_opt}, c = {c_opt}, d = {d_opt}\")\n",
        "\n",
        "# Tracer les donn√©es, la courbe ajust√©e, et le mod√®le r√©el\n",
        "y_fit = model(t_data, a_opt, b_opt, c_opt, d_opt)\n",
        "y_real = model(t_data, 2, 0.5, 1, 0)  # Mod√®le r√©el sans bruit\n",
        "plt.title(\"Methode normal\")\n",
        "plt.plot(t_data, y_data, 'o', label='Donn√©es', color='blue')\n",
        "plt.plot(t_data, y_real, '--', label='Mod√®le r√©el', color='black')\n",
        "plt.plot(t_data, y_fit, '-', label='Mod√®le ajust√©', color='red')\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Nxw2bFB1LJrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pas fix"
      ],
      "metadata": {
        "id": "Pp8QWRgTu6Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# D√©finir le mod√®le\n",
        "def modele(t, a, b, c, d):\n",
        "    return a * np.exp(-b * t) * np.cos(c * t + d)\n",
        "\n",
        "# Fonction de perte (Erreur quadratique moyenne)\n",
        "def fonction_perte(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    return np.mean((predictions - y) ** 2)\n",
        "\n",
        "# Gradient de la fonction de perte\n",
        "def gradients(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    grad_a = 2 * np.mean((predictions - y) * np.exp(-b * t) * np.cos(c * t + d))\n",
        "    grad_b = 2 * np.mean((predictions - y) * (-a * t * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    grad_c = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * t * np.sin(c * t + d)))\n",
        "    grad_d = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    return grad_a, grad_b, grad_c, grad_d\n",
        "\n",
        "# Descente de gradient avec pas fixe\n",
        "def descente_gradient_pas_fixe(t, y, a_init, b_init, c_init, d_init, taux_apprentissage, n_iterations):\n",
        "    a, b, c, d = a_init, b_init, c_init, d_init\n",
        "    historique_perte = []\n",
        "    for i in range(n_iterations):\n",
        "        grad_a, grad_b, grad_c, grad_d = gradients(t, y, a, b, c, d)\n",
        "        a -= taux_apprentissage * grad_a\n",
        "        b -= taux_apprentissage * grad_b\n",
        "        c -= taux_apprentissage * grad_c\n",
        "        d -= taux_apprentissage * grad_d\n",
        "        historique_perte.append(fonction_perte(t, y, a, b, c, d))\n",
        "    return a, b, c, d, historique_perte\n",
        "\n",
        "# G√©n√©rer des donn√©es synth√©tiques\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 300)\n",
        "a_vrai, b_vrai, c_vrai, d_vrai = 5, 1, 2, 0.5\n",
        "y_donnees = modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai) + np.random.normal(0, 0.5, len(t_donnees))\n",
        "\n",
        "# Appliquer la descente de gradient avec pas fixe\n",
        "a_gd, b_gd, c_gd, d_gd, perte_gd = descente_gradient_pas_fixe(\n",
        "    t_donnees, y_donnees, a_init=1, b_init=1, c_init=1, d_init=1, taux_apprentissage=0.01, n_iterations=1000\n",
        ")\n",
        "\n",
        "# Tracer les r√©sultats\n",
        "print(f\"Valeurs estim√©es : a = {a_gd}, b = {b_gd}, c = {c_gd}, d = {d_gd}\")\n",
        "print(f\"Valeurs reels : a = {a_vrai}, b = {b_vrai}, c = {c_vrai}, d = {d_vrai}\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Donn√©es bruit√©es\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai), label=\"Mod√®le r√©el\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, modele(t_donnees, a_gd, b_gd, c_gd, d_gd), label=\"Mod√®le (Gradient Descent)\", color='red')\n",
        "plt.title(\"Methode gradiant descent pas fix\")\n",
        "plt.xlabel(\"Temps (t)\")\n",
        "plt.ylabel(\"Valeurs (y)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p4N2pVCUoplG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il y a plusieurs raisons pour lesquelles le mod√®le estim√© par la descente de gradient (en rouge) ne correspond pas exactement au mod√®le r√©el (en noir). Voici les principales explications :\n",
        "\n",
        "1. Bruit dans les Donn√©es\n",
        "Les donn√©es utilis√©es pour entra√Æner le mod√®le contiennent du bruit (ajout√© al√©atoirement avec np.random.normal). Cela peut rendre l'ajustement des param√®tres plus difficile.\n",
        "Le bruit peut emp√™cher la descente de gradient de converger parfaitement vers les valeurs r√©elles des param√®tres.\n",
        "2. Initialisation des Param√®tres\n",
        "La descente de gradient d√©marre √† partir d'une estimation initiale des param√®tres (a=1, b=1, c=1, d=1). Si ces valeurs sont loin des param√®tres r√©els (\n",
        "a=5, b=1, c=2, d=0.5), la convergence peut √™tre lente ou partielle.\n",
        "Une mauvaise initialisation peut pi√©ger l'algorithme dans un minimum local.\n",
        "3. Taux d'Apprentissage\n",
        "Le taux d'apprentissage (ùúÇ=0.01) utilis√© dans cet exemple est fixe. Si ce taux est trop petit, la convergence est lente et peut ne pas atteindre le minimum global dans le nombre d'it√©rations sp√©cifi√©.\n",
        "Si le taux est trop grand, il peut provoquer des oscillations autour du minimum ou emp√™cher la convergence.\n",
        "4. Nombre d'It√©rations\n",
        "Le nombre d'it√©rations (1000) peut √™tre insuffisant pour que l'algorithme converge compl√®tement, surtout si le gradient diminue lentement au fur et √† mesure de l'ajustement des param√®tres.\n",
        "5. Non-lin√©arit√© et Complexit√© de la Fonction\n",
        "La fonction f(t)=aexp(‚àíbt)cos(ct+d) est hautement non-lin√©aire par rapport √† ses param√®tres. Cela rend la descente de gradient plus difficile, car l'algorithme peut avoir du mal √† trouver la direction optimale dans cet espace non-lin√©aire.\n",
        "6. Pas d'Approches Avanc√©es\n",
        "Les m√©thodes comme momentum, Adam, ou encore la descente avec pas optimal pourraient am√©liorer la performance de la descente de gradient et permettre une meilleure convergence vers les param√®tres r√©els.\n"
      ],
      "metadata": {
        "id": "n72FFFFgva-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pas Optimal**"
      ],
      "metadata": {
        "id": "wMMRA9WzxhVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# D√©finir le mod√®le\n",
        "def modele(t, a, b, c, d):\n",
        "    return a * np.exp(-b * t) * np.cos(c * t + d)\n",
        "\n",
        "# Fonction de perte (Erreur quadratique moyenne)\n",
        "def fonction_perte(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    return np.mean((predictions - y) ** 2)\n",
        "\n",
        "# Gradient de la fonction de perte\n",
        "def gradients(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    grad_a = 2 * np.mean((predictions - y) * np.exp(-b * t) * np.cos(c * t + d))\n",
        "    grad_b = 2 * np.mean((predictions - y) * (-a * t * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    grad_c = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * t * np.sin(c * t + d)))\n",
        "    grad_d = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    return grad_a, grad_b, grad_c, grad_d\n",
        "\n",
        "# Recherche pas optimal pour ajuster le taux d'apprentissage\n",
        "def recherche_pas_optimal(t, y, a, b, c, d, grad_a, grad_b, grad_c, grad_d, taux_apprentissage=1.0):\n",
        "    alpha = 0.5  # facteur de r√©duction\n",
        "    beta = 0.1   # tol√©rance\n",
        "    while fonction_perte(t, y, a - taux_apprentissage * grad_a, b - taux_apprentissage * grad_b, c - taux_apprentissage * grad_c, d - taux_apprentissage * grad_d) > fonction_perte(t, y, a, b, c, d) - beta * taux_apprentissage * (grad_a**2 + grad_b**2 + grad_c**2 + grad_d**2):\n",
        "        taux_apprentissage *= alpha\n",
        "    return taux_apprentissage\n",
        "\n",
        "def descente_gradient_recherche_pas_optimal(t, y, a_init, b_init, c_init, d_init, n_iterations):\n",
        "    a, b, c, d = a_init, b_init, c_init, d_init\n",
        "    historique_perte = []\n",
        "    for i in range(n_iterations):\n",
        "        grad_a, grad_b, grad_c, grad_d = gradients(t, y, a, b, c, d)\n",
        "        taux_apprentissage = recherche_pas_optimal(t, y, a, b, c, d, grad_a, grad_b, grad_c, grad_d)\n",
        "        a -= taux_apprentissage * grad_a\n",
        "        b -= taux_apprentissage * grad_b\n",
        "        c -= taux_apprentissage * grad_c\n",
        "        d -= taux_apprentissage * grad_d\n",
        "        historique_perte.append(fonction_perte(t, y, a, b, c, d))\n",
        "    return a, b, c, d, historique_perte\n",
        "\n",
        "# G√©n√©rer des donn√©es synth√©tiques bas√©es sur le mod√®le\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 100)\n",
        "a_vrai, b_vrai, c_vrai, d_vrai = 5, 1, 2, 0.5\n",
        "y_donnees = modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai) + np.random.normal(0, 0.5, len(t_donnees))  # Ajouter du bruit\n",
        "\n",
        "# Appliquer la descente de gradient avec recherche de ligne\n",
        "a_gd_ls, b_gd_ls, c_gd_ls, d_gd_ls, perte_gd_ls = descente_gradient_recherche_pas_optimal(t_donnees, y_donnees, a_init=1, b_init=1, c_init=1, d_init=1, n_iterations=1000)\n",
        "\n",
        "# Afficher les r√©sultats\n",
        "print(f\"Valeurs estim√©es : a = {a_gd}, b = {b_gd}, c = {c_gd}, d = {d_gd}\")\n",
        "print(f\"Valeurs reels : a = {a_vrai}, b = {b_vrai}, c = {c_vrai}, d = {d_vrai}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Donn√©es\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai), label=\"Mod√®le r√©el\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, modele(t_donnees, a_gd_ls, b_gd_ls, c_gd_ls, d_gd_ls), label=\"Descente de Gradient Pas optimal\", color='red')\n",
        "plt.title(\"Methode gradiant descent pas optimal\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H2uhbMcEvIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conjugee"
      ],
      "metadata": {
        "id": "J3It71QhZN6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# D√©finir le mod√®le\n",
        "def modele(t, a, b, c, d):\n",
        "    return a * np.exp(-b * t) * np.cos(c * t + d)\n",
        "\n",
        "# Fonction de perte (Erreur quadratique moyenne)\n",
        "def fonction_perte(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    return np.mean((predictions - y) ** 2)\n",
        "\n",
        "# Gradient de la fonction de perte\n",
        "def gradients(t, y, a, b, c, d):\n",
        "    predictions = modele(t, a, b, c, d)\n",
        "    grad_a = 2 * np.mean((predictions - y) * np.exp(-b * t) * np.cos(c * t + d))\n",
        "    grad_b = 2 * np.mean((predictions - y) * (-a * t * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    grad_c = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * t * np.sin(c * t + d)))\n",
        "    grad_d = 2 * np.mean((predictions - y) * (-a * np.exp(-b * t) * np.cos(c * t + d)))\n",
        "    return grad_a, grad_b, grad_c, grad_d\n",
        "\n",
        "# M√©thode du Gradient Conjugu√©\n",
        "def gradient_conjugue(t, y, a_init, b_init, c_init, d_init, n_iterations):\n",
        "    # Initialisation des param√®tres\n",
        "    a, b, c, d = a_init, b_init, c_init, d_init\n",
        "    grad_a, grad_b, grad_c, grad_d = gradients(t, y, a, b, c, d)\n",
        "    r = np.array([grad_a, grad_b, grad_c, grad_d])  # R√©sidu initial (gradient)\n",
        "    p = r.copy()  # Direction de recherche initiale\n",
        "    historique_perte = []\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        # Calcul de alpha (taille du pas)\n",
        "        alpha = np.dot(r, r) / np.dot(p, gradients(t, y, a, b, c, d))\n",
        "\n",
        "        # Mise √† jour des param√®tres\n",
        "        a -= alpha * grad_a\n",
        "        b -= alpha * grad_b\n",
        "        c -= alpha * grad_c\n",
        "        d -= alpha * grad_d\n",
        "\n",
        "        # Nouveau r√©sidu\n",
        "        grad_a, grad_b, grad_c, grad_d = gradients(t, y, a, b, c, d)\n",
        "        r_new = np.array([grad_a, grad_b, grad_c, grad_d])\n",
        "\n",
        "        # Calcul du facteur beta pour la mise √† jour de la direction\n",
        "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
        "        p = r_new + beta * p  # Mise √† jour de la direction de recherche\n",
        "        r = r_new\n",
        "        historique_perte.append(fonction_perte(t, y, a, b, c, d))\n",
        "    return a, b, c, d, historique_perte\n",
        "\n",
        "# G√©n√©rer des donn√©es synth√©tiques bas√©es sur le mod√®le\n",
        "np.random.seed(42)\n",
        "t_donnees = np.linspace(0, 10, 100)\n",
        "a_vrai, b_vrai, c_vrai, d_vrai = 5, 1, 2, 0.5\n",
        "y_donnees = modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai) + np.random.normal(0, 0.5, len(t_donnees))  # Ajouter du bruit\n",
        "\n",
        "# Appliquer la m√©thode du gradient conjugu√©\n",
        "a_cg, b_cg, c_cg, d_cg, perte_cg = gradient_conjugue(t_donnees, y_donnees, a_init=1, b_init=1, c_init=1, d_init=1, n_iterations=10000)\n",
        "\n",
        "# Afficher les r√©sultats\n",
        "print(f\"Valeurs estim√©es : a = {a_cg}, b = {b_cg}, c = {c_cg}, d = {d_cg}\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(t_donnees, y_donnees, label=\"Donn√©es\", color='blue', alpha=0.5)\n",
        "plt.plot(t_donnees, modele(t_donnees, a_vrai, b_vrai, c_vrai, d_vrai), label=\"Mod√®le r√©el\", color='black', linestyle='--')\n",
        "plt.plot(t_donnees, modele(t_donnees, a_cg, b_cg, c_cg, d_cg), label=\"Gradient Conjugu√©\", color='red')\n",
        "plt.title(\"M√©thode du Gradient Conjugu√©\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Aq-roIa5ZPq1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "441d28e3-a72e-40ba-a5ee-8ed332032cc4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'a_gd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c39eee011d5e>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Afficher les r√©sultats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Valeurs estim√©es : a = {a_gd}, b = {b_gd}, c = {c_gd}, d = {d_gd}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_donnees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_donnees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Donn√©es\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'a_gd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22YJ_rWYr1w3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}